{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyecto: Campañas de Marketing directo para depósitos bancarios a plazo fijo\n",
    "\n",
    "## Descripción del Proyecto y Origen del Dataset\n",
    "\n",
    "El dataset corresponde a una recopilación de datos relacionados a campañas de marketing, realizados mediante llamadas telefónicas a los clientes.\n",
    "\n",
    "El objetivo es evaluar la aceptación de un depósito a plazo fijo ofrecido por una entidad bancaria Portuguesa.\n",
    "\n",
    "Número de registros: 42211\n",
    "\n",
    "Número de variables: 16, más 1 variable adicional de salida.\n",
    "\n",
    "Path del Dataset: https://www.kaggle.com/datasets/thedevastator/bank-term-deposit-predictions/\n",
    "\n",
    "Github: https://github.com/vchirinosb/marketing-campaign/blob/main/marketing-campaign-decoders.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Decoders:\n",
    "\n",
    "### Investigación y Comparación de Modelos\n",
    "\n",
    "1. T5-Small (Text-To-Text Transfer Transformer)\n",
    "   - Arquitectura: Encoder-Decoder.\n",
    "   - Tamaño: 60 millones de parámetros.\n",
    "   - Uso: Versátil para tareas de NLP como traducción, resumen, clasificación, etc.\n",
    "   - Ventajas: Buena capacidad para múltiples tareas de NLP con un tamaño relativamente pequeño.\n",
    "   - Desventajas: Podría no ser tan potente como otros modelos más grandes para tareas específicas.\n",
    "\n",
    "2. GPT-2 (Generative Pre-trained Transformer 2)\n",
    "   - Arquitectura: Decoder-only.\n",
    "   - Tamaño: Variantes de 117M, 345M, 762M y 1.5B parámetros.\n",
    "   - Uso: Generación de texto, conversación, etc.\n",
    "   - Ventajas: Excelente para generación de texto continuo y coherente.\n",
    "   - Desventajas: Puede ser costoso en términos de recursos computacionales para variantes más grandes.\n",
    "\n",
    "3. Pegasus-XSUM\n",
    "   - Arquitectura: Encoder-Decoder.\n",
    "   - Tamaño: Aproximadamente 568 millones de parámetros.\n",
    "   - Uso: Optimizado para tareas de resumen de texto.\n",
    "   - Ventajas: Muy efectivo en la generación de resúmenes concisos y relevantes.\n",
    "   - Desventajas: Enfoque especializado puede limitar su rendimiento en otras tareas.\n",
    "4. LLaMA 3.1\n",
    "   - Arquitectura_ Basada en la arquitectura Transformer. Diseñada para tareas de procesamiento de lenguaje natural, como clasificación, regresión y generación de texto.\n",
    "   - Tamaño: 8 mil millones de parámetros.\n",
    "   - Uso: Predicción de variables de salida binarias, como la aceptación de un depósito a plazo fijo. Análisis de sentimiento y clasificación de texto. Generación de texto y resúmenes.\n",
    "   - Ventajas: Alta precisión en tareas de clasificación y regresión. Capacidad para manejar datos tabulares y de texto. Robustez ante datos faltantes o inconsistentes. Buen rendimiento en tareas de procesamiento de lenguaje natural.\n",
    "   - Desventajas: Requiere grandes cantidades de memoria y recursos computacionales.\n",
    "\n",
    "\n",
    "### Evaluación de los Requisitos del Proyecto:\n",
    "\n",
    "- Tarea principal: Predicción de aceptación de depósitos.\n",
    "- Tareas secundarias: Clasificación y resúmenes.\n",
    "\n",
    "\n",
    "### Proceso de Evaluación y Selección del Modelo:\n",
    "\n",
    "- Preparación de Datos: Cargar y explorar el dataset.\n",
    "- Preprocesar datos para que sean compatibles con el modelo.\n",
    "- Implementación de Modelos\n",
    "- Comparar resultados y seleccionar el modelo más adecuado.\n",
    "\n",
    "\n",
    "Justificación de la Elección:\n",
    "\n",
    "  Para la tarea específica de predicción de aceptación de depósitos a plazo fijo.\n",
    "- T5-Small: Podría ser una buena opción si los recursos son limitados y se requiere un enfoque multitarea.\n",
    "- GPT-2: Ideal para generación de texto pero puede no ser la mejor opción para clasificación directa sin modificaciones adicionales.\n",
    "- Pegasus-XSUM: Especializado en resumen, podría no ser adecuado para clasificación.\n",
    "- LLaMA 3.1: Potencialmente muy efectivo pero puede requerir recursos significativos.\n",
    "\n",
    "Basado en estos factores, T5-Small se presenta como una opción balanceada en términos de tamaño y capacidad para tareas de NLP variadas, incluyendo clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Implementación de Decoders:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Evaluación usando google/pegasus-xsum\n",
    "\n",
    "El modelo PEGASUS (Pretrained Generative Attention-based Sequence-to-Sequence) es un modelo de generación de texto.\n",
    "Diseñado para tareas de resumen de texto y generación de resúmenes coherentes y concisos.\n",
    "\n",
    "Se usa el modelo PEGASUS-XSum para generar resúmenes a partir de descripciones detalladas.\n",
    "\n",
    "Modelo: \"google/pegasus-xsum\"\n",
    "\n",
    "El código carga datos de clientes, crea descripciones a partir de esos datos, y luego usa el modelo PEGASUS para resumir una de las descripciones, guardando el resultado en un archivo de texto.\n",
    "\n",
    "Output: El resumen esperado no es el adecuado. Por lo cual el modelo no será seleccionado para entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primera descripción: Client age 58, job management, marital status married, education tertiary. Balance 2143 EUR, housing yes, loan no. Contacted via unknown on 5 may. Last contact duration 261 sec, campaign 1 contacts, days since last campaign -1, previous outcome unknown. Subscription status no.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo y tokenizador cargados exitosamente.\n",
      "Tiempo de tokenización: 0.0007 segundos\n",
      "Tiempo de generación del resumen: 7.3650 segundos\n",
      "Tiempo total: 7.3657 segundos\n",
      "Resumen generado: Contact details:\n",
      "El resumen ha sido guardado en summaries_pegasus_xsum.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "import time\n",
    "\n",
    "\n",
    "file_path = 'train.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def create_description(row):\n",
    "    \"\"\"\n",
    "    Crea una descripción concisa basada en los datos de una fila del DataFrame.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): Fila del DataFrame que contiene datos del cliente.\n",
    "\n",
    "    Returns:\n",
    "        str: Descripción concisa del cliente basada en los datos proporcionados.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        description = (\n",
    "            f\"Client age {row['age']}, job {row['job']}, marital status \"\n",
    "            f\"{row['marital']}, education {row['education']}. Balance \"\n",
    "            f\"{row['balance']} EUR, housing {row['housing']}, loan \"\n",
    "            f\"{row['loan']}. Contacted via {row['contact']} on {row['day']} \"\n",
    "            f\"{row['month']}. Last contact duration {row['duration']} sec, \"\n",
    "            f\"campaign {row['campaign']} contacts, days since last campaign \"\n",
    "            f\"{row['pdays']}, previous outcome {row['poutcome']}. \"\n",
    "            f\"Subscription status {row['y']}.\"\n",
    "        )\n",
    "        return description\n",
    "    except Exception as e:\n",
    "        print(f\"Error al crear la descripción para la fila: {row}\\nError: {e}\")\n",
    "        return \"Descripción no disponible\"\n",
    "\n",
    "# Aplicar la función para crear la columna 'description'\n",
    "df['description'] = df.apply(create_description, axis=1)\n",
    "\n",
    "# Seleccionar la primera descripción para propósitos de prueba.\n",
    "first_description = df['description'].iloc[0]\n",
    "print(f\"Primera descripción: {first_description}\")\n",
    "\n",
    "try:\n",
    "    # Cargar el tokenizador y el modelo PEGASUS-XSum.\n",
    "    model_name = \"google/pegasus-xsum\"\n",
    "    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "    print(\"Modelo y tokenizador cargados exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el modelo/tokenizador: {e}\")\n",
    "\n",
    "try:\n",
    "    # Tokenizar el texto\n",
    "    start_time = time.time()\n",
    "    batch = tokenizer(\n",
    "        [first_description], max_length=1024, truncation=True,\n",
    "        padding='longest', return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenization_time = time.time() - start_time\n",
    "\n",
    "    # Generar el resumen usando el modelo PEGASUS\n",
    "    start_time = time.time()\n",
    "    translated = model.generate(**batch)\n",
    "    generation_time = time.time() - start_time\n",
    "\n",
    "    # Decodificar la secuencia generada\n",
    "    summary = tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(f\"Tiempo de tokenización: {tokenization_time:.4f} segundos\")\n",
    "    print(f\"Tiempo de generación del resumen: {generation_time:.4f} segundos\")\n",
    "    print(f\"Tiempo total: {tokenization_time + generation_time:.4f} segundos\")\n",
    "    print(f\"Resumen generado: {summary}\")\n",
    "\n",
    "    # Guardar el resultado en ummaries_pegasus_xsum.txt\n",
    "    with open('summaries_pegasus_xsum.txt', 'w') as f:\n",
    "        f.write(f\"Descripción Original:\\n{first_description}\\n\")\n",
    "        f.write(f\"Resumen Generado:\\n{summary}\\n\\n\")\n",
    "\n",
    "    print(\"El resumen ha sido guardado en summaries_pegasus_xsum.txt\")\n",
    "except Exception as e:\n",
    "    print(f\"Error durante la generación del resumen o al guardar: {e}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluación usando T5-Small\n",
    "\n",
    "T5-small es una versión reducida del modelo T5, que sigue la arquitectura Transformer y trata todas las tareas de NLP como tareas de traducción de texto a texto.\n",
    "\n",
    "Uso en el Código: El modelo t5-small se utiliza para resumir descripciones largas generadas a partir de datos de clientes.\n",
    "\n",
    "Output: El modelo proporciona un resumen correcto, no el óptimo. El modelo será seleccionado para entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First description: Client ID 0. Age 58, job management, marital status married, education tertiary. Default status no, balance 2143 EUR, housing yes, loan no. Contacted via unknown on 5 may. Last contact duration 261 sec, campaign 1 contacts, days since last campaign -1, previous 0 contacts, previous outcome unknown. Subscription status no.\n",
      "Model and tokenizer loaded successfully.\n",
      "Summary generation time: 2.2947 seconds\n",
      "Generated Summary: Default status no, balance 2143 EUR, housing yes, loan no. Contacted via unknown on 5 may. Last contact duration 261 sec, campaign 1 contacts, days since last campaign -1, previous 0 contacts, previous outcome unknown.\n",
      "Summary has been saved to ummaries_t5_small.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import time\n",
    "\n",
    "\n",
    "# Cargar el dataset desde el archivo\n",
    "file_path = 'train.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def create_description(row):\n",
    "    \"\"\"\n",
    "    Crea una descripción concisa basada en la información de la fila del\n",
    "    DataFrame.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): Fila del DataFrame con información del cliente.\n",
    "\n",
    "    Returns:\n",
    "        str: Descripción del cliente o mensaje de error si ocurre una excepción.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        description = (\n",
    "            f\"Client ID {row.name}. Age {row['age']}, job {row['job']}, \"\n",
    "            f\"marital status {row['marital']}, education {row['education']}. \"\n",
    "            f\"Default status {row['default']}, balance {row['balance']} EUR, \"\n",
    "            f\"housing {row['housing']}, loan {row['loan']}. \"\n",
    "            f\"Contacted via {row['contact']} on {row['day']} {row['month']}. \"\n",
    "            f\"Last contact duration {row['duration']} sec, campaign \"\n",
    "            f\"{row['campaign']} contacts, days since last campaign \"\n",
    "            f\"{row['pdays']}, previous {row['previous']} contacts, previous \"\n",
    "            f\"outcome {row['poutcome']}. Subscription status {row['y']}.\"\n",
    "        )\n",
    "        return description\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating description for row: {row}\\nError: {e}\")\n",
    "        return \"Description unavailable\"\n",
    "\n",
    "# Aplicar la función para crear la columna 'description'\n",
    "df['description'] = df.apply(create_description, axis=1)\n",
    "\n",
    "# Seleccionar la primera descripción para fines de prueba.\n",
    "first_description = df['description'].iloc[0]\n",
    "print(f\"First description: {first_description}\")\n",
    "\n",
    "try:\n",
    "    # Cargar el tokenizador y el modelo T5.\n",
    "    model_name = \"t5-small\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model/tokenizer: {e}\")\n",
    "\n",
    "try:\n",
    "    # Preparar el texto de entrada para el modelo T5.\n",
    "    input_text = \"summarize: \" + first_description\n",
    "    input_ids = tokenizer.encode(\n",
    "        input_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "    # Generar el resumen usando el modelo T5.\n",
    "    start_time = time.time()\n",
    "    summary_ids = model.generate(\n",
    "        input_ids, max_length=150, min_length=30,\n",
    "        length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    generation_time = time.time() - start_time\n",
    "\n",
    "    # Decodificar la secuencia generada.\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Summary generation time: {generation_time:.4f} seconds\")\n",
    "    print(f\"Generated Summary: {summary}\")\n",
    "\n",
    "    # Guardar el resultado en summaries_t5_small.txt\n",
    "    with open('summaries_t5_small.txt', 'w') as f:\n",
    "        f.write(f\"Original Description:\\n{first_description}\\n\")\n",
    "        f.write(f\"Generated Summary:\\n{summary}\\n\\n\")\n",
    "\n",
    "    print(\"Summary has been saved to ummaries_t5_small.txt\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during summary generation or saving: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluación usando T5-Base\n",
    "\n",
    "T5-Base es una versión de tamaño intermedio del modelo T5.\n",
    "\n",
    "Transforma tareas de generación de texto, como el resumen, en un problema de generación de texto a partir de una entrada dada, simplificando el proceso de creación de resúmenes automáticos o respuestas a preguntas.\n",
    "\n",
    "Output: La salida obtenida de T5_base es similar a T5-small. Por lo que se optará por usar T5-small para entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First description: Client age 58, job management, marital status married, education tertiary. Balance 2143 EUR, housing yes, loan no. Contacted via unknown on 5 may. Last contact duration 261 sec, campaign 1 contacts, days since last campaign -1, previous outcome unknown. Subscription status no.\n",
      "Model and tokenizer loaded successfully.\n",
      "Summary generation time: 4.6403 seconds\n",
      "Generated Summary: client age 58, job management, marital status married, education tertiary. Contacted via unknown on 5 may.\n",
      "Summary has been saved to summaries_t5_base.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import time\n",
    "\n",
    "\n",
    "# Cargar el conjunto de datos desde el archivo.\n",
    "file_path = 'train.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def create_description(row):\n",
    "    \"\"\"\n",
    "    Crear una descripción concisa a partir de una fila del DataFrame.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): Una fila del DataFrame que contiene datos del cliente.\n",
    "\n",
    "    Returns:\n",
    "        str: Una descripción concatenada del cliente o un mensaje de error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        description = (\n",
    "            f\"Client age {row['age']}, job {row['job']}, marital status \"\n",
    "            f\"{row['marital']}, education {row['education']}. Balance \"\n",
    "            f\"{row['balance']} EUR, housing {row['housing']}, loan \"\n",
    "            f\"{row['loan']}. Contacted via {row['contact']} on {row['day']} \"\n",
    "            f\"{row['month']}. Last contact duration {row['duration']} sec, \"\n",
    "            f\"campaign {row['campaign']} contacts, days since last campaign \"\n",
    "            f\"{row['pdays']}, previous outcome {row['poutcome']}. \"\n",
    "            f\"Subscription status {row['y']}.\"\n",
    "        )\n",
    "        return description\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating description for row: {row}\\nError: {e}\")\n",
    "        return \"Description unavailable\"\n",
    "\n",
    "# Aplicar la función para crear la columna 'description'.\n",
    "df['description'] = df.apply(create_description, axis=1)\n",
    "\n",
    "# Seleccionar la primera descripción para fines de prueba.\n",
    "first_description = df['description'].iloc[0]\n",
    "print(f\"First description: {first_description}\")\n",
    "\n",
    "try:\n",
    "    # Cargar el tokenizador y el modelo T5.\n",
    "    model_name = \"t5-base\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model/tokenizer: {e}\")\n",
    "\n",
    "try:\n",
    "    # Preparar el texto de entrada para el modelo T5.\n",
    "    input_text = \"summarize: \" + first_description\n",
    "    input_ids = tokenizer.encode(\n",
    "        input_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "    # Generar el resumen usando el modelo T5.\n",
    "    start_time = time.time()\n",
    "    summary_ids = model.generate(\n",
    "        input_ids, max_length=150, min_length=30, length_penalty=2.0,\n",
    "        num_beams=4, early_stopping=True\n",
    "    )\n",
    "    generation_time = time.time() - start_time\n",
    "\n",
    "    # Decodificar la secuencia generada.\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Summary generation time: {generation_time:.4f} seconds\")\n",
    "    print(f\"Generated Summary: {summary}\")\n",
    "\n",
    "    # Guardar el resultado en summaries_t5_base.txt.\n",
    "    with open('summaries_t5_base.txt', 'w') as f:\n",
    "        f.write(f\"Original Description:\\n{first_description}\\n\")\n",
    "        f.write(f\"Generated Summary:\\n{summary}\\n\\n\")\n",
    "\n",
    "    print(\"Summary has been saved to summaries_t5_base.txt\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during summary generation or saving: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluación usando GPT2\n",
    "\n",
    "GPT-2 (Generative Pre-trained Transformer 2) es un modelo de lenguaje desarrollado por OpenAI. Es un modelo basado en la arquitectura de Transformer, diseñado para generar texto de manera coherente y contextualmente relevante\n",
    "\n",
    "El código genera descripciones textuales para cada entrada, utiliza GPT-2 para crear un resumen de una descripción específica, y guarda tanto la descripción original como el resumen en un archivo para su posterior revisión.\n",
    "\n",
    "Output: El modelo proporciona un resumen correcto, no el óptimo. El modelo será seleccionado para entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First description: Client ID 0. Age 58, job management, marital status married, education tertiary. Default status no, balance 2143 EUR, housing yes, loan no. Contacted via unknown on 5 may. Last contact duration 261 sec, campaign 1 contacts, days since last campaign -1, previous 0 contacts, previous outcome unknown. Subscription status no.\n",
      "Model and tokenizer loaded successfully.\n",
      "Summary generation time: 7.1035 seconds\n",
      "Generated Summary: Summarize this description: Client ID 0. Age 58, job management, marital status married, education tertiary. Default status no, balance 2143 EUR, housing yes, loan no. Contacted via unknown on 5 may. Last contact duration 261 sec, campaign 1 contacts, days since last campaign -1, previous 0 contacts, previous outcome unknown. Subscription status no. Contacted via unknown on 5 may. Last contact duration 261 sec, campaign 1 contacts, days since last campaign -1, previous 0 contacts, previous outcome unknown. Subscription status no. Contacted via unknown on 5 may. Last contact duration 261 sec, campaign 1 contacts, days since last campaign -1, previous 0 contacts, previous outcome unknown. Subscription\n",
      "Summary has been saved to summaries_gpt2.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import time\n",
    "\n",
    "\n",
    "# Cargar el conjunto de datos desde el archivo.\n",
    "file_path = 'train.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define a function to create concise descriptions\n",
    "def create_description(row):\n",
    "    try:\n",
    "        description = (\n",
    "            f\"Client ID {row.name}. Age {row['age']}, job {row['job']}, \"\n",
    "            f\"marital status {row['marital']}, education {row['education']}. \"\n",
    "            f\"Default status {row['default']}, balance {row['balance']} EUR, \"\n",
    "            f\"housing {row['housing']}, loan {row['loan']}. Contacted via \"\n",
    "            f\"{row['contact']} on {row['day']} {row['month']}. \"\n",
    "            f\"Last contact duration {row['duration']} sec, campaign \"\n",
    "            f\"{row['campaign']} contacts, days since last campaign \"\n",
    "            f\"{row['pdays']}, previous {row['previous']} contacts, \"\n",
    "            f\"previous outcome {row['poutcome']}. \"\n",
    "            f\"Subscription status {row['y']}.\"\n",
    "        )\n",
    "        return description\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating description for row: {row}\\nError: {e}\")\n",
    "        return \"Description unavailable\"\n",
    "\n",
    "# Aplicar la función para crear la columna 'description'.\n",
    "df['description'] = df.apply(create_description, axis=1)\n",
    "\n",
    "# Seleccionar la primera descripción para fines de prueba.\n",
    "first_description = df['description'].iloc[0]\n",
    "print(f\"First description: {first_description}\")\n",
    "\n",
    "try:\n",
    "    # Cargar el tokenizador y modelo GPT-2.\n",
    "    model_name = \"gpt2\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model/tokenizer: {e}\")\n",
    "\n",
    "try:\n",
    "    # Preparar el texto de entrada para el modelo GPT-2.\n",
    "    input_text = \"Summarize this description: \" + first_description\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Generar resumen utilizando el modelo GPT-2.\n",
    "    start_time = time.time()\n",
    "    summary_ids = model.generate(\n",
    "        input_ids, max_length=150, min_length=30, length_penalty=2.0,\n",
    "        num_beams=4, early_stopping=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    generation_time = time.time() - start_time\n",
    "\n",
    "    # Decodificar la secuencia generada.\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Summary generation time: {generation_time:.4f} seconds\")\n",
    "    print(f\"Generated Summary: {summary}\")\n",
    "\n",
    "    # Guardar los resultados en summaries_gpt2.txt\n",
    "    with open('summaries_gpt2.txt', 'w') as f:\n",
    "        f.write(f\"Original Description:\\n{first_description}\\n\")\n",
    "        f.write(f\"Generated Summary:\\n{summary}\\n\\n\")\n",
    "\n",
    "    print(\"Summary has been saved to summaries_gpt2.txt\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during summary generation or saving: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluación usando Ollama - llama3.1:8b\n",
    "\n",
    "\n",
    "En el código, LLaMA se utiliza para generar un resumen basado en las descripciones concisas de las filas del conjunto de datos. El modelo recibe el texto combinado como entrada y genera un resumen coherente y relevante. La funcionalidad de LLaMA en este caso es:\n",
    "- Análisis del texto: LLaMA analiza el texto combinado para entender el contenido y el contexto.\n",
    "- Generación del resumen: LLaMA genera un resumen coherente y relevante basado en el análisis del texto.\n",
    "\n",
    "En resumen, el código utiliza LLaMA para generar un resumen basado en descripciones concisas de filas de un conjunto de datos, lo que permite obtener una visión general coherente y relevante del contenido del conjunto de datos.\n",
    "\n",
    "Output: La respuesta generada es de utilidad. Es un modelo de decoder adecuado para la aplicación al proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: This appears to be a dataset from a collection agency or debt collector's database. Each entry represents an individual client with their demographic information, credit history, and details about recent contact attempts.\n",
      "\n",
      "Here are some insights that can be gleaned from this data:\n",
      "\n",
      "1. **Geographic distribution**: Although not explicitly mentioned in the dataset, the age ranges suggest a Western European country (e.g., Germany, France) as the clients are mostly between 20-80 years old.\n",
      "2. **Contact methods**: The majority of contacts were made via cellular phone (around 85%); telephone was used for only one client (Client ID 45209).\n",
      "3. **Age and employment**: Clients span a wide age range, but those in their mid-to-late 30s (35-39 years old) seem to be overrepresented (Clients 45198, 45201). The most common occupations are management (twice), blue-collar, technician, and retired.\n",
      "4. **Subscription status**: About half of the clients have a positive subscription status (yes), while the other half do not (no).\n",
      "5. **Contact outcomes**: Some clients had previous successful contact attempts (outcomes classified as \"success\") while others resulted in failure or another outcome (\"failure\" or \"other\").\n",
      "6. **Default and loan statuses**: Most clients are non-defaulters, but some have loans.\n",
      "7. **Days since last campaign**: The frequency of recent campaigns is evident, with most contacts happening within the past few days (up to -1 day ago).\n",
      "8. **Campaigns**: The number of contact attempts varies greatly among clients; some received a single contact while others were contacted multiple times.\n",
      "\n",
      "This dataset could be useful for further analysis or modeling in areas such as:\n",
      "\n",
      "* Predicting successful contact outcomes based on client demographics and contact history\n",
      "* Identifying factors influencing the likelihood of successful subscription status (e.g., age, occupation)\n",
      "* Developing strategies to improve contact success rates\n",
      "\n",
      "However, some limitations should be considered when working with this dataset:\n",
      "\n",
      "* **Lack of context**: Without knowing more about the company or specific campaigns, it can be challenging to understand the broader picture.\n",
      "* **Missing data**: Some columns are empty (e.g., \"previous contacts\" for Client ID 45198).\n",
      "* **Outliers and unusual values**: Certain clients have exceptional balances (Client ID 45208) or contact durations (Client ID 45205), which might require special attention.\n",
      "\n",
      "If you'd like to explore this dataset further, I'm happy to help with any specific questions or tasks!\n",
      "Summary has been saved to summaries_ollama.txt\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Cargar el conjunto de datos desde el archivo.\n",
    "file_path = 'train.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Definir una función para crear descripciones concisas.\n",
    "def create_description(row):\n",
    "    try:\n",
    "        description = (\n",
    "            f\"Client ID {row.name}. Age {row['age']}, job {row['job']}, \"\n",
    "            f\"marital status {row['marital']}, education {row['education']}. \"\n",
    "            f\"Default status {row['default']}, balance {row['balance']} EUR, \"\n",
    "            f\"housing {row['housing']}, loan {row['loan']}. Contacted via \"\n",
    "            f\"{row['contact']} on {row['day']} {row['month']}. \"\n",
    "            f\"Last contact duration {row['duration']} sec, campaign \"\n",
    "            f\"{row['campaign']} contacts, days since last campaign \"\n",
    "            f\"{row['pdays']}, previous {row['previous']} contacts, previous \"\n",
    "            f\"outcome {row['poutcome']}. Subscription status {row['y']}.\"\n",
    "        )\n",
    "        return description\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating description for row: {row}\\nError: {e}\")\n",
    "        return \"Description unavailable\"\n",
    "\n",
    "# Aplica la función para crear la columna 'descripción.\n",
    "df['description'] = df.apply(create_description, axis=1)\n",
    "\n",
    "# Combinar las descripciones en un solo texto.\n",
    "combined_description = \" \".join(df['description'])\n",
    "\n",
    "# Cargar el modelo ollama.\n",
    "modelo = 'llama3.1:8b'\n",
    "ollama.pull(modelo)\n",
    "\n",
    "# Definir el prompt.\n",
    "prompt = (\n",
    "    f\"Assess the descriptions and predict the probability of subscription: \"\n",
    "    f\"{combined_description}\"\n",
    ")\n",
    "\n",
    "# Generar resumen usando el modelo ollama\n",
    "response = ollama.generate(model=modelo, prompt=prompt)\n",
    "print(f\"Generated Summary: {response['response']}\")\n",
    "\n",
    "# Guardar el resultado en summaries_ollama.txt\n",
    "with open('summaries_ollama.txt', 'w') as f:\n",
    "    f.write(f\"Original Descriptions:\\n{combined_description}\\n\")\n",
    "    f.write(f\"Generated Summary:\\n{response['response']}\\n\\n\")\n",
    "\n",
    "print(\"Summary has been saved to summaries_ollama.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Entrenamiento de los modelos seleccionados:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Entrenamiento para T5-small:\n",
    "\n",
    "El modelo T5 (Text-to-Text Transfer Transformer) es un modelo de lenguaje desarrollado por Google Research. La versión \"small\" de T5 es una variante más compacta del modelo original T5, diseñada para hacer que el modelo sea más accesible en términos de recursos computacionales y tiempos de entrenamiento.\n",
    "\n",
    "El código está configurado para usar el modelo T5-small en una tarea de clasificación de texto, donde se genera una descripción de entrada y se clasifica si el cliente acepta una oferta o no.\n",
    "\n",
    "Muestra de Datos: Se usa una muestra del 25% del dataset para entrenamiento, con el objetivo de reducir el tiempo de entrenamiento\n",
    "\n",
    "División del Dataset: El dataset se divide en conjuntos de entrenamiento y validación. El 80% se usa para entrenamiento y el 20% para validación.\n",
    "\n",
    "Épocas: Se entrena por 1 época (puedes aumentar esto para un entrenamiento más extensivo).\n",
    "\n",
    "El modelo y el tokenizador se guardan en el directorio ./saved_t5_model y ./saved_t5_tokenizer, respectivamente.\n",
    "\n",
    "Output: El modelo de entrenamiento cumple con su objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import os\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset personalizado para la tokenización de entradas y etiquetas.\n",
    "\n",
    "    Args:\n",
    "        encodings (dict): Diccionario con las codificaciones de entrada.\n",
    "        labels (tensor): Tensores con las etiquetas.\n",
    "\n",
    "    Métodos:\n",
    "        __getitem__(idx): Devuelve un ítem en el índice especificado.\n",
    "        __len__(): Devuelve la longitud del dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: val[idx].clone().detach() for key, val in \n",
    "            self.encodings.items()\n",
    "        }\n",
    "        item['labels'] = self.labels[idx].clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def create_description(row):\n",
    "    \"\"\"\n",
    "    Crea una descripción detallada del cliente a partir de una fila del\n",
    "    DataFrame.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): Fila del DataFrame con la información del cliente.\n",
    "\n",
    "    Returns:\n",
    "        str: Descripción detallada del cliente.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        description = (\n",
    "            f\"Client ID {row.name}. Age {row['age']}, job {row['job']}, \"\n",
    "            f\"marital status {row['marital']}, education {row['education']}. \"\n",
    "            f\"Default status {row['default']}, balance {row['balance']} EUR, \"\n",
    "            f\"housing {row['housing']}, loan {row['loan']}. Contacted via \"\n",
    "            f\"{row['contact']} on {row['day']} {row['month']}. Last contact \"\n",
    "            f\"duration {row['duration']} sec, campaign {row['campaign']} \"\n",
    "            f\"contacts, days since last campaign {row['pdays']}, previous \"\n",
    "            f\"{row['previous']} contacts, previous outcome {row['poutcome']}. \"\n",
    "            f\"Subscription status {row['y']}.\"\n",
    "        )\n",
    "        return description\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating description for row: {row}\\nError: {e}\")\n",
    "        return \"Description unavailable\"\n",
    "\n",
    "if not os.path.exists('./saved_t5_model'):\n",
    "    df = pd.read_csv('train.csv')\n",
    "    df['description'] = df.apply(create_description, axis=1)\n",
    "\n",
    "    # Usar solo el 25% del dataset para entrenamiento.\n",
    "    df_sampled = df.sample(frac=0.25, random_state=42)\n",
    "    \n",
    "    text_data = df_sampled['description'].tolist()\n",
    "    labels = df_sampled['y'].apply(\n",
    "        lambda x: \"yes\" if x == 'yes' else \"no\").tolist()\n",
    "\n",
    "    model_name = 't5-small'\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text_data, return_tensors='pt', max_length=512, padding='max_length',\n",
    "        truncation=True)\n",
    "    labels = tokenizer(\n",
    "        labels, return_tensors='pt', max_length=512, padding='max_length',\n",
    "        truncation=True)['input_ids']\n",
    "\n",
    "    dataset = CustomDataset(inputs, labels)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=1,  # Reducido a 1 época para ahorrar tiempo.\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=100,  # Reducido el número de pasos de calentamiento.\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        eval_strategy='steps',\n",
    "        eval_steps=50,  # Evaluar con más frecuencia.\n",
    "        save_total_limit=1,  # Mantener solo el mejor modelo.\n",
    "        save_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        # Parada temprana.\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "    trainer.train()\n",
    "    model.save_pretrained('./saved_t5_model')\n",
    "    tokenizer.save_pretrained('./saved_t5_tokenizer')\n",
    "else:\n",
    "    model = T5ForConditionalGeneration.from_pretrained('./saved_t5_model')\n",
    "    tokenizer = T5Tokenizer.from_pretrained('./saved_t5_tokenizer')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "def generate_text(input_text):\n",
    "    \"\"\"\n",
    "    Genera texto utilizando el modelo T5 a partir de una entrada dada.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): Texto de entrada para la generación.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto generado por el modelo.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        input_text, return_tensors='pt', max_length=512, padding='max_length',\n",
    "        truncation=True)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    output = model.generate(\n",
    "        input_ids, attention_mask=attention_mask, max_length=150)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Client ID 1. Age 35, job management, marital status married, education tertiary. Default status no, balance 1000 EUR, housing yes, loan no. Contacted via cellular on 5 may. Last contact duration 300 sec, campaign 1 contacts, days since last campaign 999, previous 0 contacts, previous outcome unknown. Subscription status yes.\n",
      "Output: yes\n",
      "---\n",
      "Input: Client ID 2. Age 25, job engineer, marital status single, education university. Default status no, balance 1500 EUR, housing yes, loan yes. Contacted via cellular on 10 june. Last contact duration 200 sec, campaign 2 contacts, days since last campaign 999, previous 1 contacts, previous outcome failure. Subscription status no.\n",
      "Output: no\n",
      "---\n",
      "Input: Client ID 3. Age 40, job doctor, marital status divorced, education postgraduate. Default status no, balance 2000 EUR, housing no, loan no. Contacted via telephone on 15 july. Last contact duration 100 sec, campaign 3 contacts, days since last campaign 999, previous 2 contacts, previous outcome success. Subscription status yes.\n",
      "Output: yes\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "ejemplos = [\n",
    "    (\"Client ID 1. Age 35, job management, marital status married, education \"\n",
    "     \"tertiary. Default status no, balance 1000 EUR, housing yes, loan no. \"\n",
    "     \"Contacted via cellular on 5 may. Last contact duration 300 sec, campaign \"\n",
    "     \"1 contacts, days since last campaign 999, previous 0 contacts, previous \"\n",
    "     \"outcome unknown. Subscription status yes.\"),\n",
    "    (\"Client ID 2. Age 25, job engineer, marital status single, education \"\n",
    "     \"university. Default status no, balance 1500 EUR, housing yes, loan yes. \"\n",
    "     \"Contacted via cellular on 10 june. Last contact duration 200 sec, \"\n",
    "     \"campaign 2 contacts, days since last campaign 999, previous 1 contacts, \"\n",
    "     \"previous outcome failure. Subscription status no.\"),\n",
    "    (\"Client ID 3. Age 40, job doctor, marital status divorced, education \"\n",
    "     \"postgraduate. Default status no, balance 2000 EUR, housing no, loan no. \"\n",
    "     \"Contacted via telephone on 15 july. Last contact duration 100 sec, \"\n",
    "     \"campaign 3 contacts, days since last campaign 999, previous 2 contacts, \"\n",
    "     \"previous outcome success. Subscription status yes.\"),\n",
    "]\n",
    "\n",
    "for ejemplo in ejemplos:\n",
    "    generated_text = generate_text(ejemplo)\n",
    "    print(f'Input: {ejemplo}')\n",
    "    print(f'Output: {generated_text}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Entrenamiento para DISTILGPT2:\n",
    "\n",
    "DistilGPT-2 es una versión más ligera y eficiente del modelo GPT-2 desarrollado por OpenAI.\n",
    "\n",
    "Los modelos GPT2 y DistilGPT-2, no pueden ser entrenados localmente, a pesar de haber reducido al minimo los settings de configuración, debido a que se requiere recursos computacionales y GPU.\n",
    "\n",
    "Se adjunta a continuación el codigo de error obtenido:\n",
    "\n",
    "RuntimeError: MPS backend out of memory (MPS allocated: 13.45 GB, other allocations: 148.70 MB, max allowed: 13.57 GB). Tried to allocate 147.24 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset personalizado para la tokenización de entradas y etiquetas.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: val[idx].clone().detach() for key, val in\n",
    "            self.encodings.items()\n",
    "        }\n",
    "        item['labels'] = item['input_ids'].clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "def create_description(row):\n",
    "    \"\"\"\n",
    "    Crea una descripción detallada del cliente a partir de una fila del\n",
    "    DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        description = (\n",
    "            f\"Client ID {row.name}. Age {row['age']}, job {row['job']}, \"\n",
    "            f\"marital status {row['marital']}, education {row['education']}. \"\n",
    "            f\"Default status {row['default']}, balance {row['balance']} EUR, \"\n",
    "            f\"housing {row['housing']}, loan {row['loan']}. Contacted via \"\n",
    "            f\"{row['contact']} on {row['day']} {row['month']}. Last contact \"\n",
    "            f\"duration {row['duration']} sec, campaign {row['campaign']} \"\n",
    "            f\"contacts, days since last campaign {row['pdays']}, previous \"\n",
    "            f\"{row['previous']} contacts, previous outcome {row['poutcome']}. \"\n",
    "            f\"Subscription status {row['y']}.\"\n",
    "        )\n",
    "        return description\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating description for row: {row}\\nError: {e}\")\n",
    "        return \"Description unavailable\"\n",
    "\n",
    "# Preparar datos.\n",
    "df = pd.read_csv('train.csv')\n",
    "df['description'] = df.apply(create_description, axis=1)\n",
    "\n",
    "# Usar solo el 5% del dataset para entrenamiento inicial.\n",
    "df_sampled = df.sample(frac=0.05, random_state=42)\n",
    "\n",
    "text_data = df_sampled['description'].tolist()\n",
    "\n",
    "model_name = 'distilgpt2'  # Usar un modelo más pequeño.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Agregar un nuevo token de padding.\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tokenizar datos.\n",
    "inputs = tokenizer(\n",
    "    text_data, return_tensors='pt', max_length=64, padding='max_length',\n",
    "    truncation=True)\n",
    "\n",
    "dataset = CustomDataset(inputs)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_gpt2',\n",
    "    num_train_epochs=1,  # Reducido a 1 época para ahorrar tiempo\n",
    "    per_device_train_batch_size=1,  # Reducido el tamaño de batch a 1\n",
    "    per_device_eval_batch_size=1,  # Reducido el tamaño de batch a 1\n",
    "    warmup_steps=50,  # Reducido el número de pasos de calentamiento\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_gpt2',\n",
    "    logging_steps=10,\n",
    "    eval_steps=50,  # Evaluar con más frecuencia\n",
    "    save_total_limit=1,  # Mantener solo el mejor modelo.\n",
    "    save_steps=50,  # Guardar modelo cada 50 pasos.\n",
    "    load_best_model_at_end=True,  # Necesario para EarlyStoppingCallback.\n",
    "    eval_strategy=\"steps\",  # Estrategia de evaluación por pasos\n",
    ")\n",
    "\n",
    "# Usar DataCollator para manejar datos dinámicamente.\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False, pad_to_multiple_of=64\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "# Liberar memoria no utilizada.\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "elif torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "# Entrenar.\n",
    "trainer.train()\n",
    "\n",
    "# Guardar modelo.\n",
    "model.save_pretrained('./saved_gpt2_model')\n",
    "tokenizer.save_pretrained('./saved_gpt2_tokenizer')\n",
    "\n",
    "def generate_text(input_text):\n",
    "    \"\"\"\n",
    "    Genera texto utilizando el modelo GPT-2 a partir de una entrada dada.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        input_text, return_tensors='pt', max_length=64, padding='max_length',  # Reducido max_length a 64\n",
    "        truncation=True)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask, max_length=50)  # Ajustado para respuestas cortas\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "ejemplos = [\n",
    "    (\"Client ID 1. Age 35, job management, marital status married, education \"\n",
    "     \"tertiary. Default status no, balance 1000 EUR, housing yes, loan no. \"\n",
    "     \"Contacted via cellular on 5 may. Last contact duration 300 sec, campaign \"\n",
    "     \"1 contacts, days since last campaign 999, previous 0 contacts, previous \"\n",
    "     \"outcome unknown. Subscription status yes.\"),\n",
    "    (\"Client ID 2. Age 25, job engineer, marital status single, education \"\n",
    "     \"university. Default status no, balance 1500 EUR, housing yes, loan yes. \"\n",
    "     \"Contacted via cellular on 10 june. Last contact duration 200 sec, \"\n",
    "     \"campaign 2 contacts, days since last campaign 999, previous 1 contacts, \"\n",
    "     \"previous outcome failure. Subscription status no.\"),\n",
    "    (\"Client ID 3. Age 40, job doctor, marital status divorced, education \"\n",
    "     \"postgraduate. Default status no, balance 2000 EUR, housing no, loan no. \"\n",
    "     \"Contacted via telephone on 15 july. Last contact duration 100 sec, \"\n",
    "     \"campaign 3 contacts, days since last campaign 999, previous 2 contacts, \"\n",
    "     \"previous outcome success. Subscription status yes.\"),\n",
    "]\n",
    "\n",
    "for ejemplo in ejemplos:\n",
    "    generated_text = generate_text(ejemplo)\n",
    "    print(f'Input: {ejemplo}')\n",
    "    print(f'Output: {generated_text}')\n",
    "    print('---')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
